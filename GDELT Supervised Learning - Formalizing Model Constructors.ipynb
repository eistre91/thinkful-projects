{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dataset of your choice, select an outcome variable and then pick four or five other variables (one to two categorical, three to four continuous) to act as the basis for features. Explore the variables using the univariate and bivariate methods you've learned so far. \n",
    "\n",
    "Next, based on what you learned via your data exploration, create ten new features. Explain the reasoning behind each one.\n",
    "\n",
    "Finally, use filtering methods to select the five best features and justify your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from scipy.stats import ttest_1samp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "import csv, sqlite3\n",
    "%matplotlib inline\n",
    "\n",
    "from GDELT_utils import GDELT_columns, usecols, dtype_dict, \\\n",
    "                        cameo_dict, map_cameo_to_text, \\\n",
    "                        state_dict, mem_usage, state_heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"gdelt.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt = pd.read_sql_query(\"SELECT SQLDATE, Actor1CountryCode, Actor2CountryCode, \\\n",
    "                            Actor1Geo_CountryCode, Actor2Geo_CountryCode, \\\n",
    "                            EventRootCode, AVG(NumMentions), AVG(AvgTone) \\\n",
    "                          FROM gdelt \\\n",
    "                          WHERE SQLDATE > \\\"2017-05-01\\\" AND SQLDATE < \\\"2017-08-01\\\" \\\n",
    "                          GROUP BY SQLDATE, Actor1CountryCode, Actor2CountryCode, \\\n",
    "                          Actor1Geo_CountryCode, Actor2Geo_CountryCode, EventRootCode\", con)\n",
    "# aggregate data by\n",
    "# Date Actor1Code Actor2Code Actor1Geo_CountryCode Actor2Geo_CountryCode EventRootCode -> Mean AvgTone\n",
    "\n",
    "# Features Actor1Code Actor2Code Actor1Geo_CountryCode Actor2Geo_CountryCode NumMentions EventRootCode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest = [\"SQLDATE\",\n",
    "            \"AvgTone\", \"NumMentions\", \"EventRootCode\",\n",
    "            \"Actor1CountryCode\", \"Actor2CountryCode\",\n",
    "            \"Actor1Geo_CountryCode\", \"Actor2Geo_CountryCode\"]\n",
    "\n",
    "categories = [\"EventRootCode\",\n",
    "            \"Actor1CountryCode\", \"Actor2CountryCode\",\n",
    "            \"Actor1Geo_CountryCode\", \"Actor2Geo_CountryCode\"]\n",
    "\n",
    "for category_col in categories:\n",
    "    gdelt[category_col] = gdelt[category_col].astype('category')\n",
    "    \n",
    "gdelt['SQLDATE'] = pd.to_datetime(gdelt['SQLDATE'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt = gdelt.drop(labels=gdelt[(gdelt['EventRootCode'] == \"--\")].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_sample = gdelt.sample(frac=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_tones = gdelt_sample['AVG(AvgTone)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_tones = gdelt_sample['AVG(AvgTone)']\n",
    "def dim_reduce(column):\n",
    "    gdelt_cntry = gdelt_sample[column]\n",
    "\n",
    "    one_hot = pd.get_dummies(gdelt_cntry)\n",
    "    \n",
    "    one_hot_tone = pd.concat([gdelt_tones, one_hot], axis=1)\n",
    "\n",
    "    avg_avgtone_mean = one_hot_tone['AVG(AvgTone)'].mean()\n",
    "    avg_avgtone_std = one_hot_tone['AVG(AvgTone)'].std()\n",
    "\n",
    "    country_tones = []\n",
    "    country_info = []\n",
    "    for column in one_hot.columns:\n",
    "        temp = one_hot_tone[[column, 'AVG(AvgTone)']]\n",
    "        country = temp[temp[column] == 1]\n",
    "        if len(country) < 10:\n",
    "            #print(country)\n",
    "            continue\n",
    "        column_means = temp.groupby(column).mean()['AVG(AvgTone)']\n",
    "        country_tones.append(column_means)\n",
    "        country_info.append((column, column_means[0] - column_means[1], np.absolute(column_means[0] - column_means[1]), \n",
    "                            (temp[column].sum()), ttest_1samp(country, avg_avgtone_mean).pvalue[1]))\n",
    "\n",
    "    cntry_spec = pd.DataFrame(country_info, columns=[\"Country\", \"AvgTone_diff\", \"AvgTone_mag\", \"Num\", \"p-value\"])\n",
    "\n",
    "    low_decs = cntry_spec[((cntry_spec['p-value'] > 0.0001) & ((cntry_spec['AvgTone_mag'] < 1) & \\\n",
    "               (cntry_spec['AvgTone_diff'] < 0)))]['Country']\n",
    "\n",
    "    low_incs = cntry_spec[((cntry_spec['p-value'] > 0.0001) & ((cntry_spec['AvgTone_mag'] < 1) & \\\n",
    "               (cntry_spec['AvgTone_diff'] > 0)))]['Country']\n",
    "\n",
    "    low_p_value = cntry_spec[((cntry_spec['p-value'] > 0.0001) & ~((cntry_spec['AvgTone_mag'] < 1) & \\\n",
    "               (cntry_spec['AvgTone_diff'] < 0)) & ~((cntry_spec['AvgTone_mag'] < 1) & \\\n",
    "               (cntry_spec['AvgTone_diff'] > 0)))]['Country']\n",
    "    \n",
    "    return low_decs, low_incs, low_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_decs_A1CC, low_incs_A1CC, low_p_value_A1CC = dim_reduce('Actor1CountryCode')\n",
    "low_decs_A2CC, low_incs_A2CC, low_p_value_A2CC = dim_reduce('Actor2CountryCode')\n",
    "low_decs_A1GCC, low_incs_A1GCC, low_p_value_A1GCC = dim_reduce('Actor1Geo_CountryCode')\n",
    "low_decs_A2GCC, low_incs_A2GCC, low_p_value_A2GCC = dim_reduce('Actor2Geo_CountryCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = gdelt_sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_missing(x, low_decs, low_incs, low_p_value, translation):\n",
    "    #if x in l:\n",
    "    #    return x\n",
    "    if x == \"nan\":\n",
    "        return \"UNKNOWN\"\n",
    "    elif x in low_decs:\n",
    "        return \"LOW_DEC\"\n",
    "    elif x in low_incs:\n",
    "        return \"LOW_INCS\"\n",
    "    elif x in low_p_value:\n",
    "        return \"LOW_P_VALUE\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def unify_cats(data, category, low_decs, low_incs, low_p_value):\n",
    "    low_decs_unique = low_decs.unique()\n",
    "    low_decs_unique = low_incs.unique()\n",
    "    low_p_value_unique = low_p_value.unique()\n",
    "    data[(category + '_unify')] = data[category].astype(str) \\\n",
    "                                                .apply(lambda x: map_missing(x, low_decs_unique, \n",
    "                                                                             low_decs_unique, \n",
    "                                                                             low_p_value_unique, \n",
    "                                                                             \"OTHER\")) \\\n",
    "                                                .astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "unify_cats(model_df, 'Actor1CountryCode', low_decs_A1CC, low_incs_A1CC, low_p_value_A1CC)\n",
    "unify_cats(model_df, 'Actor2CountryCode', low_decs_A2CC, low_incs_A2CC, low_p_value_A2CC)\n",
    "unify_cats(model_df, 'Actor1Geo_CountryCode', low_decs_A1GCC, low_incs_A1GCC, low_p_value_A1GCC)\n",
    "unify_cats(model_df, 'Actor2Geo_CountryCode', low_decs_A2GCC, low_incs_A2GCC, low_p_value_A2GCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['norm_NumMentions'] = (model_df['AVG(NumMentions)'] - model_df['AVG(NumMentions)'].mean())/ \\\n",
    "                                        model_df['AVG(NumMentions)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_df = model_df.drop(['Actor1CountryCode', 'Actor2CountryCode', 'Actor1Geo_CountryCode', \n",
    "#               'Actor2Geo_CountryCode', 'AVG(NumMentions)', 'SQLDATE'], axis=1)\n",
    "\n",
    "Actor1CC_one_hot = pd.get_dummies(model_df['Actor1CountryCode_unify'], prefix=\"Actor1CountryCode\")\n",
    "Actor2CC_one_hot = pd.get_dummies(model_df['Actor2CountryCode_unify'], prefix=\"Actor2CountryCode\")\n",
    "Actor1CCGeo_one_hot = pd.get_dummies(model_df['Actor1Geo_CountryCode_unify'], prefix=\"Actor1Geo_CountryCode\")\n",
    "Actor2CCGeo_one_hot = pd.get_dummies(model_df['Actor2Geo_CountryCode_unify'], prefix=\"Actor2Geo_CountryCode\")\n",
    "EventRoot_one_hot = pd.get_dummies(model_df['EventRootCode'], prefix=\"EventRootCode\")\n",
    "\n",
    "one_hot_encoding = pd.concat([Actor1CC_one_hot, Actor2CC_one_hot, Actor1CCGeo_one_hot, \n",
    "                              Actor2CCGeo_one_hot, EventRoot_one_hot], axis=1)\n",
    "\n",
    "model_df_hot = pd.concat([model_df, one_hot_encoding], axis=1).drop(['Actor1CountryCode_unify',\n",
    "                                                                    'Actor2CountryCode_unify',\n",
    "                                                                    'Actor1Geo_CountryCode_unify',\n",
    "                                                                    'Actor2Geo_CountryCode_unify'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = model_df_hot.drop(['AVG(AvgTone)'], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(model_df_hot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "Y = train['AVG(AvgTone)'].values.reshape(-1, 1)\n",
    "X = train[feature_columns]\n",
    "regr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24057144216501747"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "regr = Ridge()\n",
    "Y = train['AVG(AvgTone)'].values.reshape(-1, 1)\n",
    "X = train[feature_columns]\n",
    "regr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24050873555027785"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_missing(x, l, translation):\n",
    "    if x in l:\n",
    "        return x\n",
    "    elif x == \"nan\":\n",
    "        return \"UNKNOWN\"\n",
    "    else:\n",
    "        return translation\n",
    "\n",
    "def unify_rare_cats(data, category, cut_off):\n",
    "    vc = data[category].value_counts()\n",
    "    past_cut_off = (vc/len(data)) > cut_off\n",
    "    remaining = list(vc[past_cut_off].index)\n",
    "    data[(category + '_unify')] = data[category].astype(str) \\\n",
    "                                                .apply(lambda x: map_missing(x, remaining, \"OTHER\")) \\\n",
    "                                                .astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_sample_rem = gdelt_sample.drop(['Actor1Geo_CountryCode', 'Actor2Geo_CountryCode', 'SQLDATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unify_rare_cats(gdelt_sample, 'Actor1CountryCode', .005)\n",
    "unify_rare_cats(gdelt_sample, 'Actor2CountryCode', .005)\n",
    "unify_rare_cats(gdelt_sample, 'Actor1Geo_CountryCode', .005)\n",
    "unify_rare_cats(gdelt_sample, 'Actor2Geo_CountryCode', .005)\n",
    "\n",
    "unify_rare_cats(gdelt_sample_rem, 'Actor1CountryCode', .005)\n",
    "unify_rare_cats(gdelt_sample_rem, 'Actor2CountryCode', .005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_sample['norm_NumMentions'] = (gdelt_sample['AVG(NumMentions)'] - gdelt_sample['AVG(NumMentions)'].mean())/ \\\n",
    "                                        gdelt_sample['AVG(NumMentions)'].std()\n",
    "    \n",
    "gdelt_sample_rem['norm_NumMentions'] = (gdelt_sample_rem['AVG(NumMentions)'] - gdelt_sample_rem['AVG(NumMentions)'].mean())/ \\\n",
    "                                        gdelt_sample_rem['AVG(NumMentions)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actor1CC_one_hot = pd.get_dummies(gdelt_sample['Actor1CountryCode_unify'], prefix=\"Actor1CountryCode\")\n",
    "Actor2CC_one_hot = pd.get_dummies(gdelt_sample['Actor2CountryCode_unify'], prefix=\"Actor2CountryCode\")\n",
    "Actor1CCGeo_one_hot = pd.get_dummies(gdelt_sample['Actor1Geo_CountryCode_unify'], prefix=\"Actor1Geo_CountryCode\")\n",
    "Actor2CCGeo_one_hot = pd.get_dummies(gdelt_sample['Actor2Geo_CountryCode_unify'], prefix=\"Actor2Geo_CountryCode\")\n",
    "EventRoot_one_hot = pd.get_dummies(gdelt_sample['EventRootCode'], prefix=\"EventRootCode\")\n",
    "\n",
    "one_hot_encoding = pd.concat([Actor1CC_one_hot, Actor2CC_one_hot, Actor1CCGeo_one_hot, \n",
    "                              Actor2CCGeo_one_hot, EventRoot_one_hot], axis=1)\n",
    "\n",
    "gdelt_sample = pd.concat([gdelt_sample, one_hot_encoding], axis=1)\n",
    "\n",
    "Actor1CC_one_hot_rem = pd.get_dummies(gdelt_sample_rem['Actor1CountryCode_unify'], prefix=\"Actor1CountryCode\")\n",
    "Actor2CC_one_hot_rem = pd.get_dummies(gdelt_sample_rem['Actor2CountryCode_unify'], prefix=\"Actor2CountryCode\")\n",
    "EventRoot_one_hot_rem = pd.get_dummies(gdelt_sample_rem['EventRootCode'], prefix=\"EventRootCode\")\n",
    "\n",
    "one_hot_encoding_rem = pd.concat([Actor1CC_one_hot_rem, Actor2CC_one_hot_rem, EventRoot_one_hot_rem], axis=1)\n",
    "\n",
    "gdelt_sample_rem = pd.concat([gdelt_sample_rem, one_hot_encoding_rem], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218524, 202)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdelt_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218524, 103)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdelt_sample_rem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = list(one_hot_encoding.columns)\n",
    "model_columns.append('norm_NumMentions')\n",
    "feature_columns = model_columns.copy()\n",
    "model_columns.append('AVG(AvgTone)')\n",
    "\n",
    "model_columns_rem = list(one_hot_encoding_rem.columns)\n",
    "model_columns_rem.append('norm_NumMentions')\n",
    "feature_columns_rem = model_columns_rem.copy()\n",
    "model_columns_rem.append('AVG(AvgTone)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_sample_m = gdelt_sample[model_columns].copy()\n",
    "\n",
    "gdelt_sample_rem_m = gdelt_sample_rem[model_columns_rem].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(gdelt_sample_m, test_size=0.25, random_state=42)\n",
    "\n",
    "train_rem, test_rem = train_test_split(gdelt_sample_rem_m, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163893, 186)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163893, 96)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared:\n",
      "0.21195525887786848\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "Y = train['AVG(AvgTone)']\n",
    "X = train[feature_columns]\n",
    "regr.fit(X, Y)\n",
    "\n",
    "print('\\nR-squared:')\n",
    "print(regr.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared:\n",
      "0.19612340075559853\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "Y = train_rem['AVG(AvgTone)']\n",
    "X = train_rem[feature_columns_rem]\n",
    "regr.fit(X, Y)\n",
    "\n",
    "print('\\nR-squared:')\n",
    "print(regr.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1          11.9990           20.34s\n",
      "         2          11.8767           20.96s\n",
      "         3          11.7423           21.06s\n",
      "         4          11.6426           20.51s\n",
      "         5          11.5372           20.56s\n",
      "         6          11.4539           20.20s\n",
      "         7          11.3692           20.15s\n",
      "         8          11.2915           20.01s\n",
      "         9          11.2241           19.67s\n",
      "        10          11.1566           19.59s\n",
      "        20          10.7166           17.05s\n",
      "        30          10.4914           14.72s\n",
      "        40          10.3520           12.51s\n",
      "        50          10.2515           10.34s\n",
      "        60          10.1728            8.22s\n",
      "        70          10.1086            6.14s\n",
      "        80          10.0549            4.07s\n",
      "        90          10.0058            2.04s\n",
      "       100           9.9647            0.00s\n",
      "\n",
      "R-squared:\n",
      "0.17984288931184345\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, verbose=True)\n",
    "Y = train['AVG(AvgTone)']\n",
    "X = train[feature_columns]\n",
    "gbr.fit(X, Y)\n",
    "\n",
    "print('\\nR-squared:')\n",
    "print(gbr.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1          11.9990           11.26s\n",
      "         2          11.8767           11.19s\n",
      "         3          11.7423           11.22s\n",
      "         4          11.6426           10.99s\n",
      "         5          11.5372           11.07s\n",
      "         6          11.4539           11.00s\n",
      "         7          11.3697           10.88s\n",
      "         8          11.2921           10.84s\n",
      "         9          11.2248           10.82s\n",
      "        10          11.1579           10.88s\n",
      "        20          10.7201            9.95s\n",
      "        30          10.4975            8.66s\n",
      "        40          10.3661            7.26s\n",
      "        50          10.2721            5.96s\n",
      "        60          10.2009            4.71s\n",
      "        70          10.1437            3.50s\n",
      "        80          10.0983            2.31s\n",
      "        90          10.0602            1.15s\n",
      "       100          10.0275            0.00s\n",
      "\n",
      "R-squared:\n",
      "0.17466966269757112\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, verbose=True)\n",
    "Y = train_rem['AVG(AvgTone)']\n",
    "X = train_rem[feature_columns_rem]\n",
    "gbr.fit(X, Y)\n",
    "\n",
    "print('\\nR-squared:')\n",
    "print(gbr.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1          11.9388           56.12s\n",
      "         2          11.7679           58.00s\n",
      "         3          11.5662            1.01m\n",
      "         4          11.4354           58.12s\n",
      "         5          11.2817           58.91s\n",
      "         6          11.1773           57.53s\n",
      "         7          11.0588           57.94s\n",
      "         8          10.9625           58.21s\n",
      "         9          10.8883           57.18s\n",
      "        10          10.8141           57.32s\n",
      "        20          10.4229           52.19s\n",
      "        30          10.2504           49.93s\n",
      "        40          10.1417           47.97s\n",
      "        50          10.0616           45.95s\n",
      "        60          10.0019           45.79s\n",
      "        70           9.9554           42.34s\n",
      "        80           9.9176           38.86s\n",
      "        90           9.8828           35.80s\n",
      "       100           9.8542           33.00s\n",
      "       200           9.6835           10.36s\n",
      "\n",
      "R-squared:\n",
      "0.20653569805723182\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=3, verbose=True, n_estimators=250)\n",
    "Y = train_rem['AVG(AvgTone)']\n",
    "X = train_rem[feature_columns_rem]\n",
    "gbr.fit(X, Y)\n",
    "\n",
    "print('\\nR-squared:')\n",
    "print(gbr.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared:\n",
      "0.19626564733644636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.8515251 ,  0.84879249,  0.42976662,  0.58719902,  0.24300694,\n",
       "        0.77631464,  0.86328756,  0.11948364, -0.57047295,  0.71796266,\n",
       "        0.43715595,  0.52781539,  0.27002181, -0.08290007,  0.69548257,\n",
       "       -0.69176825, -0.75654196, -0.39573903,  0.29765846, -0.43138564,\n",
       "        0.82775763,  0.51355615,  0.16058804,  0.23049128,  0.17102605,\n",
       "        0.31995076, -0.36282578, -0.18070882,  0.28522125, -0.77060142,\n",
       "       -1.0950767 , -0.61206347, -0.25294739, -0.4342424 , -1.14120839,\n",
       "       -0.29421253, -0.51348242, -0.07056624,  0.18572966, -0.68173321,\n",
       "        0.9156716 ,  0.67602895,  0.08100082,  0.65746073,  0.77851383,\n",
       "        0.15983953, -0.51390072,  0.61131216,  0.52273344,  0.60329033,\n",
       "        0.21400534, -0.01603856,  0.62005944, -0.73601223, -0.55837229,\n",
       "       -0.09700976,  0.3706473 ,  0.93742817,  0.56769648, -0.0497552 ,\n",
       "        0.06074978,  0.29155451, -0.49646468, -0.48462042, -0.88252197,\n",
       "       -0.93761505, -0.61455413, -0.28073163, -0.53419266, -1.13539194,\n",
       "       -0.37437787,  0.13222604,  0.19307387,  0.        ,  0.60207854,\n",
       "        0.96911463,  2.11772467,  1.71406262,  2.39110421,  1.52685412,\n",
       "        1.37886696,  0.30919488, -0.1957793 ,  0.12018822, -0.7097987 ,\n",
       "        0.0597169 , -0.89442082, -0.93263503,  0.33017866, -0.27868929,\n",
       "       -2.0759398 , -2.96015064, -1.53173759, -1.93993326, -0.12157232])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = Ridge()\n",
    "Y = train_rem['AVG(AvgTone)']\n",
    "X = train_rem[feature_columns_rem]\n",
    "regr.fit(X, Y)\n",
    "\n",
    "print('\\nR-squared:')\n",
    "print(regr.score(X, Y))\n",
    "\n",
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\sorting.py\u001b[0m in \u001b[0;36msafe_sort\u001b[1;34m(values, labels, na_sentinel, assume_unique)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[0msorter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m             \u001b[0mordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: data type not understood",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-e5ed86fe2bc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgdelt_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgdelt_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mget_iterator\u001b[1;34m(self, data, axis)\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \"\"\"\n\u001b[1;32m-> 1922\u001b[1;33m         \u001b[0msplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1923\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_group_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1924\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_get_splitter\u001b[1;34m(self, data, axis)\u001b[0m\n\u001b[0;32m   1926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1928\u001b[1;33m         \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1929\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.cache_readonly.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mgroup_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2038\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgroup_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2040\u001b[1;33m         \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_group_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_compressed_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[0mngroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_group_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_get_compressed_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_compressed_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m         \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m             group_index = get_group_index(all_labels, self.shape,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_compressed_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m         \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m             group_index = get_group_index(all_labels, self.shape,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mlabels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2748\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2749\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2750\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2751\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_make_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2765\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2766\u001b[0m                 labels, uniques = algorithms.factorize(\n\u001b[1;32m-> 2767\u001b[1;33m                     self.grouper, sort=self.sort)\n\u001b[0m\u001b[0;32m   2768\u001b[0m                 \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2769\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[1;34m(values, sort, order, na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msorting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msafe_sort\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         uniques, labels = safe_sort(uniques, labels, na_sentinel=na_sentinel,\n\u001b[1;32m--> 479\u001b[1;33m                                     assume_unique=True)\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\sorting.py\u001b[0m in \u001b[0;36msafe_sort\u001b[1;34m(values, labels, na_sentinel, assume_unique)\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# try this anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             \u001b[0mordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msort_mixed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[1;31m# labels:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\sorting.py\u001b[0m in \u001b[0;36msort_mixed\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    434\u001b[0m         str_pos = np.array([isinstance(x, string_types) for x in values],\n\u001b[0;32m    435\u001b[0m                            dtype=bool)\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[0mnums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mstr_pos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[0mstrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr_pos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "gdelt_sample.columns.to_series().groupby(gdelt_sample.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_sample = gdelt_sample.drop(['SQLDATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 218524 entries, 10869 to 2657273\n",
      "Columns: 196 entries, Actor1CountryCode to EventRootCode_20\n",
      "dtypes: category(9), float64(3), uint8(184)\n",
      "memory usage: 47.8 MB\n"
     ]
    }
   ],
   "source": [
    "gdelt_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdelt_sample['Actor1CountryCode'].dtype == 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Actor2CountryCode', 'Actor1Geo_CountryCode', 'Actor2Geo_CountryCode',\n",
       "       'EventRootCode', 'AVG(NumMentions)', 'AVG(AvgTone)',\n",
       "       'Actor1CountryCode_unify', 'Actor2CountryCode_unify',\n",
       "       'Actor1Geo_CountryCode_unify', 'Actor2Geo_CountryCode_unify',\n",
       "       ...\n",
       "       'EventRootCode_11', 'EventRootCode_12', 'EventRootCode_13',\n",
       "       'EventRootCode_14', 'EventRootCode_15', 'EventRootCode_16',\n",
       "       'EventRootCode_17', 'EventRootCode_18', 'EventRootCode_19',\n",
       "       'EventRootCode_20'],\n",
       "      dtype='object', length=195)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdelt_sample.columns.drop(['Actor1CountryCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reduce into quantile buckets\n",
    "def quantile_reduce(sample, column, pvalue, mag, count_penalty):\n",
    "    tones = sample['AVG(AvgTone)']\n",
    "    cntry = sample[column]\n",
    "\n",
    "    one_hot = pd.get_dummies(cntry)\n",
    "    one_hot_tone = pd.concat([tones, one_hot], axis=1)\n",
    "\n",
    "    avg_avgtone_mean = one_hot_tone['AVG(AvgTone)'].mean()\n",
    "\n",
    "    country_info = []\n",
    "    for column in one_hot.columns:\n",
    "        temp = one_hot_tone[[column, 'AVG(AvgTone)']]\n",
    "        country = temp[temp[column] == 1]\n",
    "        columns_mean = None\n",
    "        if len(country) < count_penalty: \n",
    "            column_means = [0, 0]\n",
    "            country_info.append((column, \n",
    "                                 0, \n",
    "                                 0, \n",
    "                                 (temp[column].sum()),\n",
    "                                 1)\n",
    "        else:\n",
    "            column_means = temp.groupby(column).mean()['AVG(AvgTone)']\n",
    "            country_info.append((column, \n",
    "                                 column_means[0] - column_means[1], \n",
    "                                 np.absolute(column_means[0] - column_means[1]), \n",
    "                                 (temp[column].sum()),\n",
    "                                 ttest_1samp(country, avg_avgtone_mean).pvalue[1]))\n",
    "\n",
    "    cntry_spec = pd.DataFrame(country_info, columns=[\"Country\", \"AvgTone_diff\", \"AvgTone_mag\", \"Num\", \"p-value\"])\n",
    "\n",
    "    low_decs = cntry_spec[((cntry_spec['p-value'] > pvalue) & ((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] < 0)))]['Country']\n",
    "\n",
    "    low_incs = cntry_spec[((cntry_spec['p-value'] > pvalue) & ((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] > 0)))]['Country']\n",
    "\n",
    "    low_p_value = cntry_spec[((cntry_spec['p-value'] > pvalue) & ~((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] < 0)) & ~((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] > 0)))]['Country']\n",
    "    \n",
    "    return low_decs, low_incs, low_p_value\n",
    "\n",
    "def map_quantile(x, low_decs, low_incs, low_p_value, translation):\n",
    "    if x == \"nan\":\n",
    "        return \"UNKNOWN\"\n",
    "    elif x in low_decs:\n",
    "        return \"LOW_DEC\"\n",
    "    elif x in low_incs:\n",
    "        return \"LOW_INCS\"\n",
    "    elif x in low_p_value:\n",
    "        return \"LOW_P_VALUE\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def unify_quantile_cats(data, category, pvalue, mag, low_decs, low_incs, low_p_value):\n",
    "    low_decs, low_incs, low_p_value = weak_reduce(data, category, pvalue, mag)\n",
    "    low_decs_unique = low_decs.unique()\n",
    "    low_decs_unique = low_incs.unique()\n",
    "    low_p_value_unique = low_p_value.unique()\n",
    "    data[(category + '_unify')] = data[category].astype(str) \\\n",
    "                                                .apply(lambda x: map_missing(x, low_decs_unique, \n",
    "                                                                             low_decs_unique, \n",
    "                                                                             low_p_value_unique, \n",
    "                                                                             \"OTHER\")) \\\n",
    "                                                .astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_reduce(sample, column, pvalue, mag, count_penalty):\n",
    "    tones = sample['AVG(AvgTone)']\n",
    "    cntry = sample[column]\n",
    "\n",
    "    one_hot = pd.get_dummies(cntry)\n",
    "    one_hot_tone = pd.concat([tones, one_hot], axis=1)\n",
    "\n",
    "    avg_avgtone_mean = one_hot_tone['AVG(AvgTone)'].mean()\n",
    "\n",
    "    country_info = []\n",
    "    for column in one_hot.columns:\n",
    "        temp = one_hot_tone[[column, 'AVG(AvgTone)']]\n",
    "        country = temp[temp[column] == 1]\n",
    "        columns_mean = None\n",
    "        if len(country) < count_penalty: \n",
    "            column_means = [0, 0]\n",
    "            country_info.append((column, \n",
    "                                 0, \n",
    "                                 0, \n",
    "                                 (temp[column].sum()),\n",
    "                                 1)\n",
    "        else:\n",
    "            column_means = temp.groupby(column).mean()['AVG(AvgTone)']\n",
    "            country_info.append((column, \n",
    "                                 column_means[0] - column_means[1], \n",
    "                                 np.absolute(column_means[0] - column_means[1]), \n",
    "                                 (temp[column].sum()),\n",
    "                                 ttest_1samp(country, avg_avgtone_mean).pvalue[1]))\n",
    "\n",
    "    cntry_spec = pd.DataFrame(country_info, columns=[\"Country\", \"AvgTone_diff\", \"AvgTone_mag\", \"Num\", \"p-value\"])\n",
    "\n",
    "    low_decs = cntry_spec[((cntry_spec['p-value'] > pvalue) & ((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] < 0)))]['Country']\n",
    "\n",
    "    low_incs = cntry_spec[((cntry_spec['p-value'] > pvalue) & ((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] > 0)))]['Country']\n",
    "\n",
    "    low_p_value = cntry_spec[((cntry_spec['p-value'] > pvalue) & ~((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] < 0)) & ~((cntry_spec['AvgTone_mag'] < mag) & \\\n",
    "               (cntry_spec['AvgTone_diff'] > 0)))]['Country']\n",
    "    \n",
    "    return low_decs, low_incs, low_p_value\n",
    "\n",
    "def map_weak(x, low_decs, low_incs, low_p_value, translation):\n",
    "    #if x in l:\n",
    "    #    return x\n",
    "    if x == \"nan\":\n",
    "        return \"UNKNOWN\"\n",
    "    elif x in low_decs:\n",
    "        return \"LOW_DEC\"\n",
    "    elif x in low_incs:\n",
    "        return \"LOW_INCS\"\n",
    "    elif x in low_p_value:\n",
    "        return \"LOW_P_VALUE\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def unify_weak_cats(data, category, pvalue, mag, count_penalty):\n",
    "    low_decs, low_incs, low_p_value = weak_reduce(data, category, pvalue, mag)\n",
    "    low_decs_unique = low_decs.unique()\n",
    "    low_decs_unique = low_incs.unique()\n",
    "    low_p_value_unique = low_p_value.unique()\n",
    "    data[(category + '_unify')] = data[category].astype(str) \\\n",
    "                                                .apply(lambda x: map_missing(x, low_decs_unique, \n",
    "                                                                             low_decs_unique, \n",
    "                                                                             low_p_value_unique, \n",
    "                                                                             \"OTHER\")) \\\n",
    "                                                .astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rare(x, l, translation):\n",
    "    if x in l:\n",
    "        return x\n",
    "    elif x == \"nan\":\n",
    "        return \"UNKNOWN\"\n",
    "    else:\n",
    "        return translation\n",
    "\n",
    "def unify_rare_cats(data, category, cut_off):\n",
    "    vc = data[category].value_counts()\n",
    "    past_cut_off = (vc/len(data)) > cut_off\n",
    "    remaining = list(vc[past_cut_off].index)\n",
    "    data[(category + '_unify')] = data[category].astype(str) \\\n",
    "                                                .apply(lambda x: map_missing(x, remaining, \"OTHER\")) \\\n",
    "                                                .astype('category')\n",
    "            \n",
    "def map_unknown(x):\n",
    "    if x == 'nan':\n",
    "        return \"UNKNOWN\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sample(frac): \n",
    "    gdelt_sample = gdelt.sample(frac=frac)\n",
    "                        .drop(['SQLDATE'], axis=1)\n",
    "    gdelt_sample['norm_NumMentions'] = (gdelt_sample['AVG(NumMentions)'] \\\n",
    "                                                - gdelt_sample['AVG(NumMentions)'].mean())/ \\\n",
    "                                        gdelt_sample['AVG(NumMentions)'].std()\n",
    "    gdelt_sample = gdelt_sample.drop(['AVG(NumMentions)'], axis=1) \n",
    "    return gdelt_sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pare(sample):\n",
    "    return sample.drop(['Actor1Geo_CountryCode', 'Actor2Geo_CountryCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive(sample, model):\n",
    "    live_samp = sample.copy()\n",
    "    \n",
    "    cat_dummies = []\n",
    "    drop_cols = []\n",
    "    for column in live_samp.columns:\n",
    "        if live_samp[column].dtype == 'category':\n",
    "            hot = pd.get_dummies(live_samp[column], prefix=column)\n",
    "            cat_dummies.append(hot)\n",
    "            drop_cols.append(column)\n",
    "            \n",
    "    live_samp.drop(drop_cols, axis=1)            \n",
    "    \n",
    "    one_hot_enc = pd.concat(cat_dummies, axis=1)\n",
    "    \n",
    "    model_samp = pd.concat([live_samp, one_hot_enc])\n",
    "    feat_cols = model_samp.columns.drop(['AVG(AvgTone)'])\n",
    "    \n",
    "    train, test = train_test_split(live_samp, test_size=0.25, random_state=42)\n",
    "    \n",
    "    Y = train['AVG(AvgTone)']\n",
    "    X = train[feat_cols]\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model, model_samp, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pared(sample, model):\n",
    "    live_samp = sample.copy()\n",
    "    live_samp = pare(live_samp)\n",
    "    \n",
    "    cat_dummies = []\n",
    "    drop_cols = []\n",
    "    for column in live_samp.columns:\n",
    "        if live_samp[column].dtype == 'category':\n",
    "            hot = pd.get_dummies(live_samp[column], prefix=column)\n",
    "            cat_dummies.append(hot)\n",
    "            drop_cols.append(column)\n",
    "            \n",
    "    live_samp.drop(drop_cols, axis=1)            \n",
    "    \n",
    "    one_hot_enc = pd.concat(cat_dummies, axis=1)\n",
    "    \n",
    "    model_samp = pd.concat([live_samp, one_hot_enc])\n",
    "    feat_cols = model_samp.columns.drop(['AVG(AvgTone)'])\n",
    "    \n",
    "    train, test = train_test_split(live_samp, test_size=0.25, random_state=42)\n",
    "    \n",
    "    Y = train['AVG(AvgTone)']\n",
    "    X = train[feat_cols]\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model, model_samp, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_URARE(sample, model, cut_off=.005):\n",
    "    live_samp = sample.copy()\n",
    "    \n",
    "    cat_dummies = []\n",
    "    drop_cols = []\n",
    "    for column in live_samp.columns:\n",
    "        if live_samp[column].dtype == 'category':\n",
    "            unify_rare_cats(live_samp, column, cut_off)\n",
    "            hot = pd.get_dummies(live_samp[column], prefix=column)\n",
    "            cat_dummies.append(hot)\n",
    "            drop_cols.append(column)\n",
    "            \n",
    "    live_samp.drop(drop_cols, axis=1)            \n",
    "    \n",
    "    one_hot_enc = pd.concat(cat_dummies, axis=1)\n",
    "    \n",
    "    model_samp = pd.concat([live_samp, one_hot_enc])\n",
    "    feat_cols = model_samp.columns.drop(['AVG(AvgTone)'])\n",
    "    \n",
    "    train, test = train_test_split(live_samp, test_size=0.25, random_state=42)\n",
    "    \n",
    "    Y = train['AVG(AvgTone)']\n",
    "    X = train[feat_cols]\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model, model_samp, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pared_URARE(sample, model, cut_off=.005):\n",
    "    live_samp = sample.copy()\n",
    "    live_samp = pare(live_samp)\n",
    "    \n",
    "    cat_dummies = []\n",
    "    drop_cols = []\n",
    "    for column in live_samp.columns:\n",
    "        if live_samp[column].dtype == 'category':\n",
    "            unify_rare_cats(live_samp, column, cut_off)\n",
    "            hot = pd.get_dummies(live_samp[column], prefix=column)\n",
    "            cat_dummies.append(hot)\n",
    "            drop_cols.append(column)\n",
    "            \n",
    "    live_samp.drop(drop_cols, axis=1)            \n",
    "    \n",
    "    one_hot_enc = pd.concat(cat_dummies, axis=1)\n",
    "    \n",
    "    model_samp = pd.concat([live_samp, one_hot_enc])\n",
    "    feat_cols = model_samp.columns.drop(['AVG(AvgTone)'])\n",
    "    \n",
    "    train, test = train_test_split(live_samp, test_size=0.25, random_state=42)\n",
    "    \n",
    "    Y = train['AVG(AvgTone)']\n",
    "    X = train[feat_cols]\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model, model_samp, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_UWEAK(sample, model, pvalue=0.0001, mag=1, count_penalty=10):\n",
    "    live_samp = sample.copy()\n",
    "    \n",
    "    cat_dummies = []\n",
    "    drop_cols = []\n",
    "    for column in live_samp.columns:\n",
    "        if live_samp[column].dtype == 'category':\n",
    "            unify_weak_cats(live_samp, column, pvalue, mag, count_penalty)\n",
    "            hot = pd.get_dummies(live_samp[column], prefix=column)\n",
    "            cat_dummies.append(hot)\n",
    "            drop_cols.append(column)\n",
    "            \n",
    "    live_samp.drop(drop_cols, axis=1)            \n",
    "    \n",
    "    one_hot_enc = pd.concat(cat_dummies, axis=1)\n",
    "    \n",
    "    model_samp = pd.concat([live_samp, one_hot_enc])\n",
    "    feat_cols = model_samp.columns.drop(['AVG(AvgTone)'])\n",
    "    \n",
    "    train, test = train_test_split(live_samp, test_size=0.25, random_state=42)\n",
    "    \n",
    "    Y = train['AVG(AvgTone)']\n",
    "    X = train[feat_cols]\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model, model_samp, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pared_UWEAK(sample, model, pvalue=0.0001, mag=1, count_penalty=10):\n",
    "    live_samp = sample.copy()\n",
    "    live_samp = pare(live_samp)\n",
    "        \n",
    "    cat_dummies = []\n",
    "    drop_cols = []\n",
    "    for column in live_samp.columns:\n",
    "        if live_samp[column].dtype == 'category':\n",
    "            unify_weak_cats(live_samp, column, pvalue, mag, count_penalty)\n",
    "            hot = pd.get_dummies(live_samp[column], prefix=column)\n",
    "            cat_dummies.append(hot)\n",
    "            drop_cols.append(column)\n",
    "            \n",
    "    live_samp.drop(drop_cols, axis=1)            \n",
    "    \n",
    "    one_hot_enc = pd.concat(cat_dummies, axis=1)\n",
    "    \n",
    "    model_samp = pd.concat([live_samp, one_hot_enc])\n",
    "    feat_cols = model_samp.columns.drop(['AVG(AvgTone)'])\n",
    "    \n",
    "    train, test = train_test_split(live_samp, test_size=0.25, random_state=42)\n",
    "    \n",
    "    Y = train['AVG(AvgTone)']\n",
    "    X = train[feat_cols]\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model, model_samp, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used with pared and naive\n",
    "def ICM_INT_sample(sample, country):\n",
    "    cntry_sample = sample[sample['Actor1CountryCode'] == country].copy()\n",
    "    cntry_sample['InternalEvent?'] = (cntry_sample['Actor2CountryCode'] == country).astype(int)\n",
    "    cntry_sample = cntry_sample.drop(['Actor1CountryCode', 'Actor2CountryCode'], axis=1)\n",
    "    return cntry_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used with pared and naive\n",
    "def ICM_SPEC_sample(sample, country):\n",
    "    cntry_sample = sample[sample['Actor1CountryCode'] == country].copy()\n",
    "    cntry_sample = cntry_sample.drop(['Actor1CountryCode'], axis=1)\n",
    "    return cntry_sample    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can only be used with naive\n",
    "def ICM_INT_GINT_sample(sample, country):\n",
    "    cntry_sample = sample[sample['Actor1CountryCode'] == country].copy()\n",
    "    cntry_sample['InternalEvent?'] = (cntry_sample['Actor2CountryCode'] != country).astype(int)\n",
    "    cntry_sample['Actor1AtHome?'] = (cntry_sample['Actor1Geo_CountryCode'] == country).astype(int)\n",
    "    cntry_sample['Actor2AtActor1Home?'] = (cntry_sample['Actor2Geo_CountryCode'] == country).astype(int)\n",
    "    cntry_sample = cntry_sample.drop(['Actor1CountryCode', 'Actor2CountryCode',\n",
    "                                     'Actor1Geo_CountryCode', 'Actor2Geo_CountryCode']\n",
    "                                     , axis=1)\n",
    "    return cntry_sample    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can only be used with naive\n",
    "def ICM_INT_GSPEC_sample(sample, country):\n",
    "    cntry_sample = sample[sample['Actor1CountryCode'] == country].copy()\n",
    "    cntry_sample['InternalEvent?'] = (cntry_sample['Actor2CountryCode'] != country).astype(int)\n",
    "    cntry_sample['Actor2AtActor1Home?'] = (cntry_sample['Actor2Geo_CountryCode'] == country).astype(int)\n",
    "    cntry_sample = cntry_sample.drop(['Actor1CountryCode', 'Actor2CountryCode', \n",
    "                                      'Actor2Geo_CountryCode']\n",
    "                                     , axis=1)\n",
    "    return cntry_sample    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive - Actor1CC, Actor2CC, Actor1GCC, Actor2GCC, EventRootCode, NumMentions\n",
    "pared - Actor1CC, Actor2CC, EventRootCode, NumMentions\n",
    "\n",
    "w/o infrequent columns naive \n",
    "w/o infrequent columns pared\n",
    "\n",
    "w/o low effect countries naive \n",
    "w/o low effect countries pared\n",
    "\n",
    "pared ICM-INT - individualized country models for Actor1CC - Actor2CC international or not\n",
    "pared ICM-SPEC - individualized country models for Actor1CC - Actor2CC maintains value\n",
    "\n",
    "naive ICM-INT/ICM-SPEC - as before but with Actor1GCC and Actor2GCC unchanged\n",
    "naive ICM-INT-INT - both Actor1GCC/Actor2GCC code internal or international\n",
    "naive ICM-INT-SPEC - Actor1GCC maintains location of Actor1, Actor2GCC codes international\n",
    "\n",
    "Actor2AtActor1Home vs Actor2AtHome\n",
    "\n",
    "quantile buckets countries naive\n",
    "quantile buckets countries pared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
