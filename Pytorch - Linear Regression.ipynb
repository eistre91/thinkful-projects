{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient 2\n",
    "Bias/Intercept 1\n",
    "Equation y = 2x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [i for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = [2 * i + 1 for i in x_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dimension is x, output dimension is y\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE Loss: Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta = \\theta - \\eta * \\nabla_\\theta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta$ : parameters (our variables)\n",
    "\n",
    "$\\eta$ : learning rate (how fast we want to learn)\n",
    "\n",
    "$\\nabla_\\theta$ : parameters' gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or\n",
    "\n",
    "parameters = parameters - learning_rate * parameters_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process\n",
    "\n",
    "1. Convert inputs/labels to variables\n",
    "2. Clear gradient buffets (buffers?)\n",
    "3. Get output given inputs\n",
    "4. Get loss\n",
    "5. Get gradients w.r.t. parameters\n",
    "6. Update parameters using gradients\n",
    "7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 177.41822814941406\n",
      "epoch 2, loss 14.528583526611328\n",
      "epoch 3, loss 1.2415492534637451\n",
      "epoch 4, loss 0.1571379154920578\n",
      "epoch 5, loss 0.06806215643882751\n",
      "epoch 6, loss 0.060179565101861954\n",
      "epoch 7, loss 0.058926597237586975\n",
      "epoch 8, loss 0.05822118744254112\n",
      "epoch 9, loss 0.05756707489490509\n",
      "epoch 10, loss 0.056923963129520416\n",
      "epoch 11, loss 0.056288279592990875\n",
      "epoch 12, loss 0.05565967410802841\n",
      "epoch 13, loss 0.05503818020224571\n",
      "epoch 14, loss 0.054423537105321884\n",
      "epoch 15, loss 0.053815748542547226\n",
      "epoch 16, loss 0.053214870393276215\n",
      "epoch 17, loss 0.05262066051363945\n",
      "epoch 18, loss 0.05203293636441231\n",
      "epoch 19, loss 0.05145193636417389\n",
      "epoch 20, loss 0.050877440720796585\n",
      "epoch 21, loss 0.05030927434563637\n",
      "epoch 22, loss 0.04974745586514473\n",
      "epoch 23, loss 0.049191948026418686\n",
      "epoch 24, loss 0.048642635345458984\n",
      "epoch 25, loss 0.04809940233826637\n",
      "epoch 26, loss 0.04756234586238861\n",
      "epoch 27, loss 0.047031186521053314\n",
      "epoch 28, loss 0.04650605097413063\n",
      "epoch 29, loss 0.04598665609955788\n",
      "epoch 30, loss 0.04547320306301117\n",
      "epoch 31, loss 0.04496540129184723\n",
      "epoch 32, loss 0.04446323961019516\n",
      "epoch 33, loss 0.043966714292764664\n",
      "epoch 34, loss 0.04347585141658783\n",
      "epoch 35, loss 0.042990364134311676\n",
      "epoch 36, loss 0.042510226368904114\n",
      "epoch 37, loss 0.04203548654913902\n",
      "epoch 38, loss 0.04156612232327461\n",
      "epoch 39, loss 0.041101936250925064\n",
      "epoch 40, loss 0.04064292460680008\n",
      "epoch 41, loss 0.040189120918512344\n",
      "epoch 42, loss 0.03974029794335365\n",
      "epoch 43, loss 0.03929654136300087\n",
      "epoch 44, loss 0.038857702165842056\n",
      "epoch 45, loss 0.038423843681812286\n",
      "epoch 46, loss 0.03799471631646156\n",
      "epoch 47, loss 0.0375705361366272\n",
      "epoch 48, loss 0.03715091571211815\n",
      "epoch 49, loss 0.03673611581325531\n",
      "epoch 50, loss 0.03632582351565361\n",
      "epoch 51, loss 0.03592021390795708\n",
      "epoch 52, loss 0.035519156605005264\n",
      "epoch 53, loss 0.03512239456176758\n",
      "epoch 54, loss 0.03473027050495148\n",
      "epoch 55, loss 0.03434237837791443\n",
      "epoch 56, loss 0.033958934247493744\n",
      "epoch 57, loss 0.033579763025045395\n",
      "epoch 58, loss 0.033204689621925354\n",
      "epoch 59, loss 0.032833993434906006\n",
      "epoch 60, loss 0.03246729075908661\n",
      "epoch 61, loss 0.03210467845201492\n",
      "epoch 62, loss 0.03174624219536781\n",
      "epoch 63, loss 0.031391680240631104\n",
      "epoch 64, loss 0.0310410987585783\n",
      "epoch 65, loss 0.03069448284804821\n",
      "epoch 66, loss 0.030351735651493073\n",
      "epoch 67, loss 0.03001284785568714\n",
      "epoch 68, loss 0.029677676036953926\n",
      "epoch 69, loss 0.029346255585551262\n",
      "epoch 70, loss 0.029018539935350418\n",
      "epoch 71, loss 0.02869442105293274\n",
      "epoch 72, loss 0.028374146670103073\n",
      "epoch 73, loss 0.028057264164090157\n",
      "epoch 74, loss 0.027743857353925705\n",
      "epoch 75, loss 0.027434123679995537\n",
      "epoch 76, loss 0.027127785608172417\n",
      "epoch 77, loss 0.0268248338252306\n",
      "epoch 78, loss 0.026525311172008514\n",
      "epoch 79, loss 0.026229066774249077\n",
      "epoch 80, loss 0.025936132296919823\n",
      "epoch 81, loss 0.025646572932600975\n",
      "epoch 82, loss 0.02536015398800373\n",
      "epoch 83, loss 0.025077007710933685\n",
      "epoch 84, loss 0.0247968602925539\n",
      "epoch 85, loss 0.024520007893443108\n",
      "epoch 86, loss 0.024246210232377052\n",
      "epoch 87, loss 0.023975437507033348\n",
      "epoch 88, loss 0.02370765618979931\n",
      "epoch 89, loss 0.023442985489964485\n",
      "epoch 90, loss 0.023181241005659103\n",
      "epoch 91, loss 0.022922368720173836\n",
      "epoch 92, loss 0.022666342556476593\n",
      "epoch 93, loss 0.022413231432437897\n",
      "epoch 94, loss 0.022162988781929016\n",
      "epoch 95, loss 0.02191552333533764\n",
      "epoch 96, loss 0.021670721471309662\n",
      "epoch 97, loss 0.021428750827908516\n",
      "epoch 98, loss 0.021189488470554352\n",
      "epoch 99, loss 0.020952828228473663\n",
      "epoch 100, loss 0.02071884088218212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e.istre91\\Anaconda3\\envs\\pyt-gpu\\lib\\site-packages\\ipykernel\\__main__.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    \n",
    "    # Convert numpy array to torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7322416],\n",
       "       [ 2.7708013],\n",
       "       [ 4.809361 ],\n",
       "       [ 6.8479204],\n",
       "       [ 8.88648  ],\n",
       "       [10.92504  ],\n",
       "       [12.963599 ],\n",
       "       [15.002159 ],\n",
       "       [17.040718 ],\n",
       "       [19.079277 ],\n",
       "       [21.117838 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model is True:\n",
    "    # Saves only parameters\n",
    "    # alpha & beta\n",
    "    torch.save(model.state_dict(), 'awesome_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('awesome_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "2 things must be on gpu\n",
    " - model\n",
    " - variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 191.89048767089844\n",
      "epoch 2, loss 16.033597946166992\n",
      "epoch 3, loss 1.6852434873580933\n",
      "epoch 4, loss 0.5106783509254456\n",
      "epoch 5, loss 0.41070544719696045\n",
      "epoch 6, loss 0.3984297811985016\n",
      "epoch 7, loss 0.3933533728122711\n",
      "epoch 8, loss 0.38890978693962097\n",
      "epoch 9, loss 0.3845627009868622\n",
      "epoch 10, loss 0.3802678883075714\n",
      "epoch 11, loss 0.3760213851928711\n",
      "epoch 12, loss 0.37182244658470154\n",
      "epoch 13, loss 0.36767056584358215\n",
      "epoch 14, loss 0.36356475949287415\n",
      "epoch 15, loss 0.3595049977302551\n",
      "epoch 16, loss 0.3554903566837311\n",
      "epoch 17, loss 0.3515208065509796\n",
      "epoch 18, loss 0.3475952446460724\n",
      "epoch 19, loss 0.3437137305736542\n",
      "epoch 20, loss 0.3398754894733429\n",
      "epoch 21, loss 0.3360801935195923\n",
      "epoch 22, loss 0.33232709765434265\n",
      "epoch 23, loss 0.3286161422729492\n",
      "epoch 24, loss 0.3249466121196747\n",
      "epoch 25, loss 0.3213178217411041\n",
      "epoch 26, loss 0.3177298605442047\n",
      "epoch 27, loss 0.3141818046569824\n",
      "epoch 28, loss 0.3106732666492462\n",
      "epoch 29, loss 0.30720406770706177\n",
      "epoch 30, loss 0.30377358198165894\n",
      "epoch 31, loss 0.30038151144981384\n",
      "epoch 32, loss 0.29702723026275635\n",
      "epoch 33, loss 0.29371029138565063\n",
      "epoch 34, loss 0.29043054580688477\n",
      "epoch 35, loss 0.2871873080730438\n",
      "epoch 36, loss 0.28398022055625916\n",
      "epoch 37, loss 0.2808091342449188\n",
      "epoch 38, loss 0.27767327427864075\n",
      "epoch 39, loss 0.2745726406574249\n",
      "epoch 40, loss 0.2715063691139221\n",
      "epoch 41, loss 0.2684747278690338\n",
      "epoch 42, loss 0.26547667384147644\n",
      "epoch 43, loss 0.2625121474266052\n",
      "epoch 44, loss 0.2595806419849396\n",
      "epoch 45, loss 0.25668197870254517\n",
      "epoch 46, loss 0.2538156509399414\n",
      "epoch 47, loss 0.25098130106925964\n",
      "epoch 48, loss 0.24817867577075958\n",
      "epoch 49, loss 0.24540728330612183\n",
      "epoch 50, loss 0.24266691505908966\n",
      "epoch 51, loss 0.23995696008205414\n",
      "epoch 52, loss 0.23727752268314362\n",
      "epoch 53, loss 0.23462794721126556\n",
      "epoch 54, loss 0.23200789093971252\n",
      "epoch 55, loss 0.2294171005487442\n",
      "epoch 56, loss 0.2268550843000412\n",
      "epoch 57, loss 0.2243218868970871\n",
      "epoch 58, loss 0.22181688249111176\n",
      "epoch 59, loss 0.21933989226818085\n",
      "epoch 60, loss 0.21689067780971527\n",
      "epoch 61, loss 0.21446853876113892\n",
      "epoch 62, loss 0.21207374334335327\n",
      "epoch 63, loss 0.20970533788204193\n",
      "epoch 64, loss 0.20736375451087952\n",
      "epoch 65, loss 0.20504814386367798\n",
      "epoch 66, loss 0.20275841653347015\n",
      "epoch 67, loss 0.20049422979354858\n",
      "epoch 68, loss 0.19825531542301178\n",
      "epoch 69, loss 0.196041539311409\n",
      "epoch 70, loss 0.1938522905111313\n",
      "epoch 71, loss 0.19168740510940552\n",
      "epoch 72, loss 0.18954697251319885\n",
      "epoch 73, loss 0.1874304860830307\n",
      "epoch 74, loss 0.1853373497724533\n",
      "epoch 75, loss 0.183267742395401\n",
      "epoch 76, loss 0.18122118711471558\n",
      "epoch 77, loss 0.17919762432575226\n",
      "epoch 78, loss 0.17719650268554688\n",
      "epoch 79, loss 0.17521770298480988\n",
      "epoch 80, loss 0.1732611507177353\n",
      "epoch 81, loss 0.1713263839483261\n",
      "epoch 82, loss 0.1694132387638092\n",
      "epoch 83, loss 0.1675213724374771\n",
      "epoch 84, loss 0.1656506210565567\n",
      "epoch 85, loss 0.16380098462104797\n",
      "epoch 86, loss 0.16197173297405243\n",
      "epoch 87, loss 0.16016311943531036\n",
      "epoch 88, loss 0.15837451815605164\n",
      "epoch 89, loss 0.1566060185432434\n",
      "epoch 90, loss 0.15485729277133942\n",
      "epoch 91, loss 0.1531279981136322\n",
      "epoch 92, loss 0.15141794085502625\n",
      "epoch 93, loss 0.14972707629203796\n",
      "epoch 94, loss 0.14805512130260468\n",
      "epoch 95, loss 0.14640189707279205\n",
      "epoch 96, loss 0.14476698637008667\n",
      "epoch 97, loss 0.14315031468868256\n",
      "epoch 98, loss 0.14155183732509613\n",
      "epoch 99, loss 0.13997116684913635\n",
      "epoch 100, loss 0.13840824365615845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e.istre91\\Anaconda3\\envs\\pyt-gpu\\lib\\site-packages\\ipykernel\\__main__.py:91: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "'''\n",
    "CREATE MODEL CLASS\n",
    "'''\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "'''\n",
    "INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "TRAIN THE MODEL\n",
    "'''\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # Convert numpy array to torch Variable\n",
    "    \n",
    "    #######################\n",
    "    #  USE GPU FOR MODEL  #\n",
    "    #######################\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        \n",
    "    #######################\n",
    "    #  USE GPU FOR MODEL  #\n",
    "    #######################\n",
    "    if torch.cuda.is_available():\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "        \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Logging\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyt-gpu]",
   "language": "python",
   "name": "conda-env-pyt-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
