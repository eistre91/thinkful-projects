{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath(\"../Thinkful Projects/Corpus of Presential Speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob(os.path.join(PATH, \"**/*.txt\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "president_files = {}\n",
    "for president in os.listdir(PATH):\n",
    "    SOURCE = os.path.join(PATH, president)\n",
    "    for file in os.listdir(SOURCE):\n",
    "        if president in president_files:\n",
    "            p = os.path.join(SOURCE, file)\n",
    "            president_files[president].append(p)\n",
    "        else:\n",
    "            p = os.path.join(SOURCE, file)\n",
    "            president_files[president] = [p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ex = re.compile('<.*?>')\n",
    "pres_texts = {}\n",
    "for pres, file_list in president_files.items():\n",
    "    pres_text = \"\"\n",
    "    for file in file_list:\n",
    "        f = open(file, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        cleaned = reg_ex.sub('', text).strip()\n",
    "        pres_text = pres_text + \" \" + cleaned\n",
    "        f.close()\n",
    "    pres_texts[pres] = pres_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en', disable=['tagger', 'ner'])\n",
    "#nlp.max_length = 1500000 \n",
    "#docs = [[pres, nlp(text)] for pres, text in pres_texts.items()]\n",
    "#pickle.dump(docs, open(\"parsed_docs\", \"wb\"))\n",
    "\n",
    "docs = pickle.load(open(\"parsed_docs\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_and_pres = [[sent, pres] for pres, doc in docs for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = pd.DataFrame(sent_and_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "\n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame()\n",
    "    for word in common_words:\n",
    "        df[word] = pd.Series(data=[0] * len(sentences),\n",
    "            index=pd.RangeIndex(len(sentences)), \n",
    "            dtype=np.uint8)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1].astype('category')\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "    \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.at[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "bags = [bag_of_words(doc) for pres, doc in docs]\n",
    "\n",
    "common_words_bag = set([word for bag in bags for word in bag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 10000\n",
      "Processing row 20000\n",
      "Processing row 30000\n",
      "Processing row 40000\n",
      "Processing row 50000\n",
      "Processing row 60000\n",
      "Processing row 70000\n",
      "Processing row 80000\n",
      "Processing row 90000\n",
      "Processing row 100000\n",
      "Processing row 110000\n",
      "Processing row 120000\n",
      "Processing row 130000\n"
     ]
    }
   ],
   "source": [
    "word_counts = bow_features(df_sent, common_words_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137048 entries, 0 to 137047\n",
      "Columns: 9820 entries, Formosa to text_source\n",
      "dtypes: category(1), object(1), uint16(9818)\n",
      "memory usage: 2.5+ GB\n"
     ]
    }
   ],
   "source": [
    "word_counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Formosa</th>\n",
       "      <th>Palestinian</th>\n",
       "      <th>strike</th>\n",
       "      <th>central</th>\n",
       "      <th>consolation</th>\n",
       "      <th>slide</th>\n",
       "      <th>pork</th>\n",
       "      <th>Manufacturers</th>\n",
       "      <th>33</th>\n",
       "      <th>debtor</th>\n",
       "      <th>...</th>\n",
       "      <th>fitness</th>\n",
       "      <th>emphasis</th>\n",
       "      <th>Rev.</th>\n",
       "      <th>GI</th>\n",
       "      <th>nonproductive</th>\n",
       "      <th>overall</th>\n",
       "      <th>Alzheimer</th>\n",
       "      <th>inch</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>( , The, personal, inconveniences, to, the, me...</td>\n",
       "      <td>adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(It, would, have, afforded, me, the, highest, ...</td>\n",
       "      <td>adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(While, other, states, are, desolated, with, f...</td>\n",
       "      <td>adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(It, is, with, extreme, regret, that, I, shall...</td>\n",
       "      <td>adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(But, if, the, tide, of, our, prosperity, is, ...</td>\n",
       "      <td>adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Formosa  Palestinian  strike  central  consolation  slide  pork  \\\n",
       "0        0            0       0        0            0      0     0   \n",
       "1        0            0       0        0            0      0     0   \n",
       "2        0            0       0        0            0      0     0   \n",
       "3        0            0       0        0            0      0     0   \n",
       "4        0            0       0        0            0      0     0   \n",
       "\n",
       "   Manufacturers  33  debtor     ...       fitness  emphasis  Rev.  GI  \\\n",
       "0              0   0       0     ...             0         0     0   0   \n",
       "1              0   0       0     ...             0         0     0   0   \n",
       "2              0   0       0     ...             0         0     0   0   \n",
       "3              0   0       0     ...             0         0     0   0   \n",
       "4              0   0       0     ...             0         0     0   0   \n",
       "\n",
       "   nonproductive  overall  Alzheimer  inch  \\\n",
       "0              0        0          0     0   \n",
       "1              0        0          0     0   \n",
       "2              0        0          0     0   \n",
       "3              0        0          0     0   \n",
       "4              0        0          0     0   \n",
       "\n",
       "                                       text_sentence  text_source  \n",
       "0  ( , The, personal, inconveniences, to, the, me...        adams  \n",
       "1  (It, would, have, afforded, me, the, highest, ...        adams  \n",
       "2  (While, other, states, are, desolated, with, f...        adams  \n",
       "3  (It, is, with, extreme, regret, that, I, shall...        adams  \n",
       "4  (But, if, the, tide, of, our, prosperity, is, ...        adams  \n",
       "\n",
       "[5 rows x 9820 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_counts.dtypes.to_dict()\n",
    "#pickle.dump(word_counts.dtypes.to_dict(), open(\"word_counts_dtypes\", \"wb\"))\n",
    "\n",
    "word_count_dtypes = pickle.load(open(\"word_counts_dtypes\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_counts.to_csv(\"psa_word_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_chunks = []\n",
    "#for chunk in pd.read_csv(\"psa_word_counts.csv\", \n",
    "#                         dtype=word_count_dtypes,\n",
    "#                        chunksize=5000):\n",
    "#    df_chunks.append(chunk)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_10_pres = [\"obama\", \"gwbush\", \"bush\", \"clinton\", \"reagan\", \n",
    "                \"carter\", \"ford\", \"nixon\", \"johnson\", \"kennedy\"]\n",
    "\n",
    "pres_data = word_counts[word_counts[\"text_source\"].isin(last_10_pres)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurs = pres_data.drop([\"text_sentence\", \"text_source\"],axis=1).sum()\n",
    "occurs = occurs[(occurs > 10)].index.append(pd.Index([\"text_source\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_pres_data = pres_data.filter(items=occurs, axis=1)\n",
    "#for column in slice_pres_data.columns:\n",
    "#    if slice_pres_data[column].dtype == np.uint16:\n",
    "#        slice_pres_data[column] = slice_pres_data[column].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = slice_pres_data.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55068 entries, 6933 to 118376\n",
      "Columns: 9820 entries, Formosa to text_source\n",
      "dtypes: category(1), object(1), uint16(9818)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "pres_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = slice_pres_data.drop(['text_source'], axis=1)\n",
    "Y = slice_pres_data['text_source'].cat.remove_unused_categories()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obama      0.190492\n",
       "reagan     0.181721\n",
       "clinton    0.132908\n",
       "gwbush     0.109501\n",
       "kennedy    0.101565\n",
       "bush       0.078557\n",
       "carter     0.063394\n",
       "nixon      0.053570\n",
       "johnson    0.050792\n",
       "ford       0.037499\n",
       "Name: text_source, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts() / len(slice_pres_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model would guess Obama for every speech for accuracy of .19 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41301, 4561) (41301,)\n",
      "Training set score: 0.6400813539623738\n",
      "\n",
      "Test set score: 0.4860899251834096\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obama      0.190492\n",
      "reagan     0.181721\n",
      "clinton    0.132908\n",
      "gwbush     0.109501\n",
      "kennedy    0.101565\n",
      "bush       0.078557\n",
      "carter     0.063394\n",
      "nixon      0.053570\n",
      "johnson    0.050792\n",
      "ford       0.037499\n",
      "Name: text_source, dtype: float64\n",
      "(41301, 9818) (41301,)\n",
      "Training set score: 0.6721144766470546\n",
      "\n",
      "Test set score: 0.493862134088763\n"
     ]
    }
   ],
   "source": [
    "X = pres_data.drop(['text_source', 'text_sentence'], axis=1)\n",
    "Y = pres_data['text_source'].cat.remove_unused_categories()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                             min_df=10, \n",
    "                             stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',\n",
    "                             smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_and_pres = [[sent, pres] for pres, doc in docs for sent in doc.sents]\n",
    "sents = [sent.text for sent, pres in sent_and_pres if pres in last_10_pres]\n",
    "pres_for_sents = [pres for sent, pres in sent_and_pres if pres in last_10_pres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_data_tfidf = pd.DataFrame(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_data_tfidf['pres'] = pres_for_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pres_data_tfidf.drop(['pres'], axis=1)\n",
    "Y = pres_data_tfidf['pres']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41301, 5779) (41301,)\n",
      "Training set score: 0.573230672380814\n",
      "\n",
      "Test set score: 0.45659911382290985\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
