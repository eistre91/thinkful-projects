{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Description\n",
    "\n",
    "Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "\n",
    "    Do any of your classifiers seem to overfit?\n",
    "    Which seem to perform the best? Why?\n",
    "    Which features seemed to be most impactful to performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_amazon = \"amazon_cells_labelled.txt\"\n",
    "amazon_raw = pd.read_csv(data_amazon, sep='\\t', header=None)\n",
    "amazon_raw.columns = ['sentence', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate\n",
    "word_dict = {}\n",
    "punc_trans = str.maketrans('', '', string.punctuation)\n",
    "num_trans = str.maketrans('', '', string.digits)\n",
    "for sentence in amazon_raw['sentence']:\n",
    "    words = [word.lower() \\\n",
    "             .translate(punc_trans) \\\n",
    "             .translate(num_trans) \\\n",
    "             .strip()\n",
    "             for word in sentence.split(' ')]\n",
    "    for word in words:\n",
    "        word_dict[word] = word_dict.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           128\n",
       "a          218\n",
       "abhor        1\n",
       "ability      2\n",
       "able         4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_series = pd.Series(word_dict)\n",
    "word_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['great', 'good', 'quality', 'recommend', 'excellent',\n",
    "           'best', 'like', 'nice']\n",
    "stretch_keys = ['great', 'good', 'quality', 'recommend', 'excellent',\n",
    "           'best', 'service', 'price', 'like', 'nice', 'works', 'work',\n",
    "               'dont', 'price', 'really', 'service']\n",
    "all_words = list(word_series.drop('').index)\n",
    "\n",
    "for key in all_words:\n",
    "    amazon_raw[str(key)] = amazon_raw.sentence.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_raw['score'] = (amazon_raw['score'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I refine the \"greedy tuning\" workflow from the first challenge by incorporating cross validation as a model's score as well as a final hold out test set that is only seen after cross validation decides which model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = amazon_raw['score']\n",
    "\n",
    "CONSTANT_SPLIT = 42\n",
    "def greedy_tuning(start_error_rate, keys, cap=None):\n",
    "    if cap == None:\n",
    "        cap = (len(keys))\n",
    "    bnb_best = None   \n",
    "    best_score_mean = 0\n",
    "    greedy_list = []\n",
    "    test_keys = keys.copy()\n",
    "    for i in range(cap):\n",
    "        best_add = None\n",
    "        for keyword in test_keys:\n",
    "            test_list = greedy_list.copy()\n",
    "            test_list.append(keyword)\n",
    "            data = amazon_raw[test_list]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                data, target, test_size=0.1, random_state=CONSTANT_SPLIT)\n",
    "            bnb = BernoulliNB()\n",
    "            scores = cross_val_score(bnb, X_train, y_train, cv=5)\n",
    "            score_mean = scores.mean()\n",
    "            if score_mean > best_score_mean:\n",
    "                best_add, best_score_mean = keyword, score_mean\n",
    "                bnb_best = bnb\n",
    "        if best_add == None:\n",
    "            break\n",
    "        else:\n",
    "            test_keys.remove(best_add)\n",
    "            greedy_list.append(best_add)\n",
    "    data = amazon_raw[greedy_list]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                data, target, test_size=0.1, random_state=CONSTANT_SPLIT)\n",
    "    bnb_best.fit(X_train, y_train)\n",
    "    y_pred = bnb_best.predict(X_test)\n",
    "    print(greedy_list)\n",
    "    print(\"CV Score: \", best_score_mean)\n",
    "    print(\"Score on Test Set: \", (y_pred != y_test).sum()/len(y_test))\n",
    "    true_pos = ((y_test == 1) & (y_pred == 1)).sum()\n",
    "    true_neg = ((y_test == 0) & (y_pred == 0)).sum()\n",
    "    false_pos = ((y_test == 0) & (y_pred == 1)).sum()\n",
    "    false_neg = ((y_test == 1) & (y_pred == 0)).sum()\n",
    "\n",
    "    specificity = true_neg / (true_neg + false_pos)\n",
    "    sensitivity = true_pos / (true_pos + false_neg)\n",
    "\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(true_neg, false_neg)\n",
    "    print(false_pos, true_pos)\n",
    "\n",
    "    print(\"Sensitivity: \", sensitivity)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    return greedy_list, bnb_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the function with the same parameters as before, we get the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'good', 'best', 'recommend', 'like', 'excellent', 'nice']\n",
      "CV Score:  0.6022222222222222\n",
      "Score on Test Set:  0.41\n",
      "Confusion Matrix: \n",
      "46 37\n",
      "4 13\n",
      "Sensitivity:  0.26\n",
      "Specificity:  0.92\n",
      "['great', 'works', 'good', 'best', 'recommend', 'price', 'like', 'excellent', 'nice']\n",
      "CV Score:  0.6288888888888888\n",
      "Score on Test Set:  0.38\n",
      "Confusion Matrix: \n",
      "46 34\n",
      "4 16\n",
      "Sensitivity:  0.32\n",
      "Specificity:  0.92\n",
      "['not', 'of', 'that', 'only', 'then', 'too', 'and', 'you', 'waste', 'if', 'completely', 'had', 'works', 'buy', 'difficult', 'make', 'old', 'plug', 'anything', 'disappointed', 'price', 'bad', 'best', 'after', 'service', 'buying', 'blackberry', 'came', 'customer', 'being', 'cases', 'plugged', 'color', 'definitely', 'there', 'finally', 'pull', 'good', 'will', 'worst', 'reviews', 'return', 'job', 'hate', 'keyboard', 'wasted', 'area', 'pretty', 'enough', 'any', 'forced', 'hours', 'consumer', 'razr', 'below']\n",
      "CV Score:  0.78\n",
      "Score on Test Set:  0.39\n",
      "Confusion Matrix: \n",
      "22 11\n",
      "28 39\n",
      "Sensitivity:  0.78\n",
      "Specificity:  0.44\n",
      "['not', 'of', 'that', 'only', 'then', 'too', 'and', 'you', 'waste', 'if']\n",
      "CV Score:  0.6622222222222222\n",
      "Score on Test Set:  0.4\n",
      "Confusion Matrix: \n",
      "17 7\n",
      "33 43\n",
      "Sensitivity:  0.86\n",
      "Specificity:  0.34\n"
     ]
    }
   ],
   "source": [
    "greedy_list_min, bnb_min = greedy_tuning(1000, keywords)\n",
    "greedy_list_med, bnb_med = greedy_tuning(1000, stretch_keys)\n",
    "greedy_list_all, bnb_all = greedy_tuning(1000, all_words)\n",
    "greedy_list_cap, bnb_cap = greedy_tuning(1000, all_words, cap=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison the old results are as follows, on a test set 20% of the size total data with a random seed of 42:\n",
    "\n",
    "```\n",
    "['good', 'quality', 'recommend', 'excellent', 'best', 'great'] 0.405\n",
    "Confusion Matrix: \n",
    "89 82\n",
    "4 25\n",
    "Sensitivity:  0.2336448598130841\n",
    "Specificity:  0.956989247311828\n",
    "['good', 'works', 'quality', 'recommend', 'price', 'best'] 0.355\n",
    "Confusion Matrix: \n",
    "90 68\n",
    "3 39\n",
    "Sensitivity:  0.3644859813084112\n",
    "Specificity:  0.967741935483871\n",
    "['not', 'get', 'buy', 'me', 'right', 'do', 'last', 'return', 'service', 'terrible', 'too', 'ask', 'bad', 'cannot'] 0.255\n",
    "Confusion Matrix: \n",
    "44 4\n",
    "49 103\n",
    "Sensitivity:  0.9626168224299065\n",
    "Specificity:  0.4731182795698925\n",
    "['not', 'get', 'buy', 'me', 'right', 'do', 'last', 'return', 'service', 'terrible'] 0.28\n",
    "Confusion Matrix: \n",
    "40 5\n",
    "53 102\n",
    "Sensitivity:  0.9532710280373832\n",
    "Specificity:  0.43010752688172044\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that cross validation encourages the classifier to use more words and has slightly better balance in sensitivity and specificity. It however, does not match the same performance, albeit on a different test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Optimizer for balancing sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec_improvers = ['good', 'quality', 'recommend', 'excellent', 'best', 'great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good 77\n",
      "good\n",
      "False    0.486316\n",
      "True     0.760000\n",
      "Name: score, dtype: float64\n",
      "quality 49\n",
      "quality\n",
      "False    0.495346\n",
      "True     0.636364\n",
      "Name: score, dtype: float64\n",
      "recommend 26\n",
      "recommend\n",
      "False    0.493852\n",
      "True     0.750000\n",
      "Name: score, dtype: float64\n",
      "excellent 27\n",
      "excellent\n",
      "False    0.495968\n",
      "True     1.000000\n",
      "Name: score, dtype: float64\n",
      "best 23\n",
      "best\n",
      "False    0.492386\n",
      "True     1.000000\n",
      "Name: score, dtype: float64\n",
      "great 97\n",
      "great\n",
      "False    0.483971\n",
      "True     0.969697\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for imp in spec_improvers:\n",
    "    print(imp, word_series[imp])\n",
    "    print(amazon_raw.groupby(imp)['score'].mean())\n",
    "    \n",
    "# idea, greedy tuner which takes penalties for imbalanced \n",
    "# specificity and sensitivity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
